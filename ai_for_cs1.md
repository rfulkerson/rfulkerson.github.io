<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://github.com/rfulkerson/25spring1400/blob/main/ai_for_cs1.md">Using AI Responsibly for CS1</a> &copy; 2025 by <span property="cc:attributionName">Robert Fulkerson</span> is licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" vel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>

**Last updated January 18th, 2026. This is a work in progress.**

# Using AI Responsibly for CS1

What we learn in a CS1 course -- all of the basic syntax, concepts, and computational thinking -- is foundational material for understanding how to think about solving problems with programming, write your own code, and debug your own code as well as code that others have written. In the ever-evolving world of Artificial Intelligence (AI) and Large Language Models (LLMs), hereafter simply referred to as AI, it also means working with AI to create, analyze, and debug code generated by those tools which can write solutions as small as ten lines or as large as thousands of lines of code in mere seconds. 

As you're beginning your computing careers in CS1, relying on AI to write code for you can undermine learning those foundational topics.

If we think about professions that rely on computing technology such as the medical field, banking, or aeronautics, it becomes readily apparent why we need people who are experts in and understand the fundamentals of coding as well as the subject matter itself. AI can certainly assist in any of those fields, but the foundational knowledge in both areas -- CS and the profession itself -- are necessary to develop and support robust software solutions in those industries.

For example, in the medical field, hardware that relies on software can be categorized as [Software in a Medical Device (SiMD)](https://www.freyrsolutions.com/what-is-software-in-a-medical-device-simd) and [Software as a Medical Device (SaMD)](https://www.fda.gov/medical-devices/digital-health-center-excellence/software-medical-device-samd). SiMD can mean software that controls devices like [infusion pumps](https://www.fda.gov/medical-devices/infusion-pumps/what-infusion-pump), [implantable pacemakers](https://pmc.ncbi.nlm.nih.gov/articles/PMC6174378/#:~:text=Closed%E2%80%90loop%20stimulation%20(CLS),responds%20with%20rapid%20atrioventricular%20pacing.), or [MRI machines](https://americanhealthimaging.com/what-to-expect-from-different-types-of-mri-machines/). SaMD could be an app that uses a smartphone's microphone to detect [interrupted breathing during sleep](https://www.mayoclinic.org/diseases-conditions/obstructive-sleep-apnea/symptoms-causes/syc-20352090), software that analyzes heart signals or patterns to aid in the diagnosis of [arrhythmia](https://www.mayoclinic.org/diseases-conditions/heart-arrhythmia/symptoms-causes/syc-20350668), or software that processes imaging data for [cancer detection](https://ieeexplore.ieee.org/document/10059977). Would you trust software written solely by AI based on generative prompts written by a medical or computer science professional who doesn't have a solid foundation in both fields? Would you prefer that the professional have experience in both medicine and computer science/programming in order to be able to develop, test, analyze, and verify useful and trustworthy software?

Thinking about this in a different way, 

Since the release of ChatGPT in 2022 and its meteoric rise to widespread adoption and acceptance in many industries and sectors, including education, many students have seen AI as a simple *solution engine*, pasting assignments into an AI engine and having it generate passable and oftentimes correct solutions that they then submit as "homework". With the rapid proliferation of more AI engines such as Claude, Gemini, and Github Copilot, it's incredibly easy to get assistance with code now.

While you will undoubtedly use AI as a collaboration tool at some point later in you academic career, as a beginning CS student you should consider using AI as a *suggestion engine* instead of a solution engine. Your role as a computer scientist and Computer Science 1 (CS1) student is to verify, adapt, and learn from those suggestions and not take the solutions generated by AI wholly at face value.

---

## Tasks that AI can be useful for in CS1

Unless the teacher explicitly states that a student can or should use AI on an assignment, there is a disconnect with what is intended by the instructor and how the student is engaging with the material if AI is used to complete the assignment without much, if any, work on the student's part. Copying and pasting assignments into an AI bypasses the carefully structured assignment developed by the teacher, likely intended to either scaffold the student to the next level or assess their current understanding of the material.

If AI is not explicitly allowed or approved for use on an assignment or in a course, there are still a number of ways that AI can be used productively and responsibly for learning materials in a CS1 course without using it as a solution engine but rather as a suggestion engine.

Through careful, intentional prompt creation, AI can be used to augment without replacing your learning of the foundational computational and programmatic material in a typical CS1 course. Click on a task area below for more information and to see example AI prompts that you can modify for your own use.

Task | Usefulness | Basic Tips
:-- | :-- | :--
[Research a topic further](#research)|  Practice prompting, interacting with results, and evaluating sources. |  Cross-check at least one claim outside of the AI/LLM to deepen your understanding.
<a href="#assigns">Assignment guidance without code</a> |  AI can outline tasks and make suggestions about how to approach a problem. |   Analyze your own thinking against what the AI suggests.
[Interactive quiz / review](#review) |  Great for getting different versions of questions about topics you're learning. |  Frame the review in terms of CS1 and cross-check questions.
[Generate debugging practice problems](#debugging) |  Debugging is a core CS1 skill. |  Ask the LLM to not label or identify bugs.
[Brainstorm personal projects](#projects) |  Personal interest drives active and engaged learning and leads to rewarding results. |   Frame idea generation in terms of CS1 skills and have the AI evaluate your project code.
Stubbed-out larger project |  Mimics industry practice of inheriting partial codebases. |   Frame in terms of CS1 skills, have AI create project for you to complete.

---

### <a name="research">Using AI to Research a Topic Further</a>

There are times when a textbook, practice problems, recitation discussions, and lecture participation may not completely flush out a topic in CS1. Or you might find yourself having questions about a topic that go beyond what is covered in the course. By structuring AI prompts carefully, you can ask for assistance or more information without having the AI directly write code or a solution for you. 

Here are some typical scenarios and example prompts you could use as guidance for your own questions.

Goal |  Example prompt 
:-- | :--
Concept clarification |  Can you explain the concept of type in Python? Assume I only know CS1 materials.
Debugging help | I'm getting a TypeError when I run this code. Can you help me understand what's wrong?<pre>`grades = [88, 92, 79]` <br>`average = 0`<br>`for g in grades:`<br>`    average += g`<br>`    print("Average: " + average / len(grades))`</pre>
Design guidance | I need to store student scores. Which built-in Python type would be most appropriate and why?
Step-by-step hint | I'm stuck on converting a string like `'1,149'` to an integer. Don't give me the final code, but can you nudge me in the right direction?

Further thoughts on prompt creation for topic research:

* **Emphasize minimal, runnable examples.** Paste only what's necessary so that the feedback is focused.
* **Ask for explanations first, code only if necessary.** "Explain why my loop is off-by-one before suggesting a fix."
* **Compare AI advice to official docs.** Cross-check functions/code/results with official documentation/requirements, building healthy skepticism.
* **Reflect aloud.** After the AI's answer, paraphrase out loud what your takeaway is. ([Rubber duck debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging))


---

### <a name="assigns">Assignment Guidance</a>

Probably the most widely used aspect of AI today for a CS1 student is copying and pasting an assignment's specifications into an AI to have it simply write code to solve the problem for them.

There's zero learning in that situation, and you're not preparing yourself for more advanced coursework int he future that will require you to think deeply and critically about how to implement a solution to a problem or how to analyze content that is generated by an AI assistant.

*A different way to approach this situation is to use the AI as a suggestion engine and competent CS tutor.*

Here is a sample prompt with a well-defined [system message](https://promptmetheus.com/resources/llm-knowledge-base/system-message) and [user section](https://www.nebuly.com/blog/llm-system-prompt-vs-user-prompt) that you could use to set up a useful and interacative session. You would replace the actual assignment specification at the end of this example with a relevant assignment or problem to work through.

>You are a friendly CS1-level Python tutor. Your job is to guide me through the problem-solving process *without providing any code, pseudocode, or step-by-step algorithmic solution*.
>
> * Use plain language appropriate for a first-semester programming student.
> * Limit jargon; if a technical term is necessary, give a short definition in parentheses.
> * Favor concrete examples that involve basic Python concepts (variables, loops, lists, functions) and avoid advanced topics (classes, recursion, decorators, exceptions) unless I explicitly ask.
> * Encourage me to think aloud and verify my own understanding.
> * End each reply with one "check-your-understanding" question I can answer before we continue.
>
> What I need from you:
> 
> 1. Plain-English summary – Restate the task so I can confirm my understanding.
> 2.	Key elements – List inputs, outputs, and constraints.
> 3.	Conceptual roadmap – Suggest high-level strategies or data structures I might explore (no code).
> 4.	Guiding questions – Ask me what I should consider (edge cases, tests, pitfalls).
> 5.	Forbidden content – No code, pseudocode, or exact algorithmic steps.
> 6.	Check-back option – Tell me how I can follow up once I've tried something.
> 7.	(Reminder) – Please keep explanations at a CS1 beginner level.
>
> Assignment Description:
> 
> An acronym is a word formed from the initial letters of words in a given phrase. Write a program whose input is a phrase and whose output is an appropriate acronym of the input. Append a period (`.`) after each letter in the acronym. Acronyms should only be built from words that start with capital letters. You can assume the input has at least one word that starts with upper case letter.
> 
> Ex: If the input is:
>
> `As Soon As Possible`
>
> the output is:
>
> `A.S.A.P.`
>
> Ex: If the input is:
>
> `Completely Automated Public Turing tEST to tell COMPUTERS and Humans Apart`
>
> the output is:
> 
> `C.A.P.T.C.H.A.`
>
> Although the letters `OMPUTERS` in `COMPUTERS` are upper case, those letters are not processed for the acronym for being a part of the word `COMPUTERS`. Similarly, even though `EST` in the word `tEST` are capitalized, they are not processed because the program should only pay attention to the first letter of the word.

Here's are some reasons that a prompt like this works so well:

Item/Directive | Purpose/Result
:-- | :--
Initial system message |Gives the model a hard boundary: "No code."
"CS1-level Python tutor" | Signals the target audience and expected depth.
Plain-English summary | Lets you verify that you and the AI interpret the prompt the same way.
Key elements | Helps you isolate inputs/outputs before thinking about design.
Conceptual roadmap | Encourages AI to generate content prompting you to think in [abstractions](https://www.learning.com/blog/abstraction-in-computational-thinking/) (loops, lists, dictionaries) without being given answers.
Guiding questions | Promotes [metacognition](https://tll.mit.edu/teaching-resources/how-people-learn/metacognition/#:~:text=Metacognition%20is%20the%20process%20by%20which%20learners%20use%20knowledge%20of%20the%20task%20at%20hand%2C%20knowledge%20of%20learning%20strategies%2C%20and%20knowledge%20of%20themselves%20to%20plan%20their%20learning%2C%20monitor%20their%20progress%20towards%20a%20learning%20goal%2C%20and%20then%20evaluate%20the%20outcome.%C2%A0) and a checklist mindset.
Forbidden content | Reinforces the no-code rule and reduces policy slip-ups.
Jargon limiter + parenthetical defs | Prevents overwhelm and builds vocabulary gradually.
Scope boundaries (avoid classes/recursion/exceptions) | Keeps the discussion aligned with the course.
Check-your-understanding question | Promotes active learning and gives you a natural pause to reflect.
Beginner-level reminder in User section | Redundancy that further reduces drift into advanced territory.

Here are some tips you can keep in mind while working with this type of prompt:

1.	**Iterate safely** – If the model starts drifting into code, you should be able to redirect it with: 
"Please remove any code from future responses and stick to conceptual guidance."
1. **Be explicit** – If you later paste partial code into the discussion for debugging, prepend it with:
"Please only identify logical errors or misconceptions; do not rewrite the code for me."
3.	**Cross-verify** – You should critically compare the AI's initial summary with the original specification. Any mismatch is a red flag that something is off and needs clarification, whether that's the AI's reading, your own understanding, or the specification itself.
4.	**Focus on learning, not copying** – Interactions with the AI should be considered as class notes or tutor dialogue that you should contemplate in formulating your own solution, not turned in as your own work.

Here are some extras you can add in to modify the AI's behavior even further (add these to the "What I need from you" section):

1.	**Word-count ceiling**: "Keep each answer under 250 words so that I'm not flooded with information."
2.	**[Bloom's Taxonomy](https://www.youtube.com/watch?v=ve-Evb5bGoc) scaffolding**: "Frame your guiding questions to progress from recall → apply → analyze."
3.	**Tone cue**: "Use a warm, encouraging tone and celebrate small insights."

---

### <a name="review">Interactive Quiz / Review</a>

Using an AI for quiz or concept review can be done simply or in a more detailed manner. The prompt you use can guide your experience with the review.

Here's an example of a simple prompt that generates some questions for review:

> Hi. I'm taking a CS1 course and we have a quiz coming up soon about functions and strings. Can you prompt me with about 15 questions appropriate for a CS1 course and check my correctness?

And here's a more detailed prompt that specifies exact parameters of the content and the types of questions. The topics section would be modified for the specific material being studied.

> I'm studying for a CS1 quiz that will cover functions, strings, lists, and dictionaries in Python. Could you create a 15-question practice quiz that mixes multiple-choice, short-answer, code-writing, and conceptual/explanation questions focused on key concepts from these topics?
>
> Topics should include but not be limited to:
> * Defining and calling functions
> * Parameters, arguments, and return values
> * Default parameters and keyword arguments
> * Returning multiple values from a function
> * String manipulation (e.g., indexing, slicing, concatenation, etc.)
> * String methods (e.g., upper(), lower(), strip(), etc.)
> * Lists: indexing, appending, slicing, and basic iteration
> * Dictionaries: creating key-value pairs, accessing/modifying values, basic iteration
> * Conceptual understanding of how functions, strings, lists, and dictionaries behave in Python (e.g., parameter passing, immutability vs. mutability, reference vs. copy, what it means to return a value, etc.)
> 
> Please exclude questions involving error handling or try/except blocks, as those are not covered at this level.
>
> After each question, please allow me to answer and provide feedback on my response before moving on to the next question.
> 
> If presenting code, please format it with multiple lines for clarity.
>
> At the end, please provide a summary of my performance with suggestions on areas to review and areas of strength.

---

### <a name="debugging">Guided Debugging Practice</a>

According to [O'Dell](https://queue.acm.org/detail.cfm?id=3068754) (2017), studies have shown that software engineers spend between 35-50% of their on-the-job time validating and debugging code, and those activities absorb 50-70% of a project's entire budget. [Saini](https://www.blackduck.com/blog/cost-to-fix-bugs-during-each-sdlc-phase.html) (2017) adds about the software development lifecycle (SDLC) that, "prevention is better than a cure, and this definitely holds true when it comes to bugs and security issues. During the development process, it is more cost-effective and efficient to fix bugs in earlier stages rather than later ones. The cost of fixing an issue increases exponentially as the software moves forward in the SDLC."

For these and many other reasons, it's critical to develop good debugging skills early on in your computer science career. There are many times during development of initial programs in a CS1 course that students may get frustrated when their code generates an error. It's eas to know when a syntax error or runtime error occurs because the code doesn't run or crashes while running. But the error messages that are generated from either type of error can be cryptic and difficult to decipher. Logic errors, when the code runs to completion but generates incorrect results, are even tougher to troubleshoot.

AI can help generate CS1-appropriate interactive debugging practice problems without simply plugging in code that's not working and asking it to be fixed.

Here is a prompt template you can use to generate an interactive debugging tutoring session:

> You are an encouraging CS1 Python tutor. Your job is to create a short buggy program and guide me through finding and fixing the bug(s) without giving the corrected code until I have attempted a solution or explicitly ask for it.
> 
> * Use language appropriate for an absolute beginner.
> * Keep each message under 200 words so the chat stays snappy.
>
> Follow the interaction instructions below exactly:
>
> DIFFICULTY = "easy | medium | tricky"<br/>
> CONCEPT_FOCUS = "strings | loops | lists | functions | file-I/O | …"<br/>
> BUG_TYPE = "syntax | runtime | logic"<br/>
> COMMONNESS = "common | less-common | rare"<br/>
> LOC_LIMIT = "≤10 | 15–20 | ~30><br/>
> BUG_COUNT = "one bug | two unrelated bugs | several cascading bugs"<br/>
> ERROR_VISIBILITY = "visible traceback | silent wrong output"<br/>
> SCAFFOLDING = "docstring only | sample I/O | failing unit test"<br/>
> COMMENT_QUALITY = "accurate comments | misleading comment | no comments"
>
> Your tasks:
> 
>	1.	Generate the buggy snippet that fits all the parameters above.
>	2.	Show any expected output / failing test that belongs to the scaffolding level.
>	3.	Invite me to inspect the code and describe what I think the bug is.
>	4.	When I respond, ask probing questions or offer incremental hints (maximum one hint per reply) until I either:
>	    *	supply a fix, or
>	    *	say "show me the answer."
>	5.	Once I've fixed it (or request the answer), provide the corrected code and a brief explanation of the fix.
>	6.	End with a reflection question ("What will you look for first the next time you see a bug like this?").
>
> Forbidden:
> * Don't reveal the full solution until step 5.
> * Don't introduce concepts beyond CS1 unless I ask.

For the "configuration block", you would choose one of the options for each category, as seen below. You can use the suggestions above or try modifying and adapting them to your liking.

> DIFFICULTY = "medium"<br/>
> CONCEPT_FOCUS = "loops"<br/>
> BUG_TYPE = "runtime"<br/>
> COMMONNESS = "common"<br/>
> LOC_LIMIT = "15–20"<br/>
> BUG_COUNT = "one bug"<br/>
> ERROR_VISIBILITY = "visible traceback"<br/>
> SCAFFOLDING = "sample I/O"<br/>
> COMMENT_QUALITY = "accurate comments"

Here are some notes about some of the configuration options:

* `DIFFICULTY`: Difficulty refers to how difficult you would like the generated debugging issue to be.
* `CONCEPT_FOCUS`: This can be a list of topics you would like the error to focus on. Limit the number of topics if you would like to focus on a particular concept.
* `BUGTYPE`: You can guide the AI to create a syntax, runtime, or logic error.
* `COMMONNESS`: This refers to how common the error is to encounter as a CS1 student. For example, `common` would generate errors that CS1 students might encounter quite often while writing code. 
* `LOC_LIMIT`: LOC stands of Lines of Code, so you can specify about how big you'd like the code to be.
* `BUG_COUNT`: This can describe how many bugs you would like generated.
* `ERROR_VISIBILITY`: This helps guide the AI to generate a stack trace (`visible traceback`, showing errors that would be generated by Python with syntax or runtime errors) or keep that information hidden while trying to debug the problem.
* `COMMENT_QUALITY`: This guides the AI to generate no comments, accurate comments, or misleading comments (all situations you may encounter as a professional developer).
* `SCAFFOLDING`: Scaffolding refers to the support that the AI provides to get you started. Here is a table of possible scaffolding options:

Value to plug into the prompt |What the model will add |When you might use it
:-- | :-- |:--
none|Just the buggy code block, "no explanation, no I/O examples.|Later in the term or when you're more confident about your skills.
docstring only |A short docstring describing the function's intent.|Quick comprehension check without giving away behavior.
inline comments|Comments (accurate) explaining what each section should do.|Good first diagnostic practice.
misleading comment|Incorrect comment that conflicts with the code's real intent.|Forces you to question assumptions.
sample I/O|One or two example input->output pairs|You can run & compare results quickly.
expected output block|A full multiline output the program should produce.|Helpful for longer scripts (file-I/O, text menus).
doctest lines|Python >>> doctest examples that currently fail.|Integrates testing mindset early.
failing unit test|A tiny pytest test suite that fails.|Introduces formal testing tools; great prep for later courses.
logging scaffold|Pre-inserted `print()` / logging calls that reveal state.|Teaches trace-based debugging techniques.
stack-trace only|The traceback captured from a run (but no sample input).|Focuses on reading error messages.
partial skeleton|Function signatures and TODO comments; body is missing or wrong.|Mirrors how real bug tickets often arrive.

A complete example prompt might look like this:

> You are an encouraging CS1 Python tutor. Your job is to create a short buggy program and guide me through finding and fixing the bug(s) without giving the corrected code until I have attempted a solution or explicitly ask for it.
> 
> * Use language appropriate for an absolute beginner.
> * Keep each message under 200 words so the chat stays snappy.
> * Follow the interaction instructions below exactly.
>
> DIFFICULTY = "medium"<br/>
> CONCEPT_FOCUS = "loops"<br/>
> BUG_TYPE = "runtime"<br/>
> COMMONNESS = "common"<br/>
> LOC_LIMIT = "15–20"<br/>
> BUG_COUNT = "one bug"<br/>
> ERROR_VISIBILITY = "visible traceback"<br/>
> SCAFFOLDING = "sample I/O"<br/>
> COMMENT_QUALITY = "accurate comments"
> 
> Your tasks:
> 
> 1. Generate the buggy snippet that fits all the parameters above.
> 2. Show any expected output / failing test that belongs to the scaffolding level.
> 3.  Invite me to inspect the code and describe what I think the bug is.
> 4. When I respond, ask probing questions or offer incremental hints (maximum one hint per reply) until I either:
>     * supply a fix, or
>     * say "show me the answer."
> 5. Once I've fixed it (or request the answer), provide the corrected code and a brief explanation of the fix.
> 6. End with a reflection question ("What will you look for first the next time you see a bug like this?").
> 
> Forbidden:
> * Don't reveal the full solution until step 5.
> * Don't introduce concepts beyond CS1 unless I ask.

---

### <a name="projects">Generating Personal Project Ideas</a>

Depending on your CS1 course, you may create larger projects that go beyond simple syntax and integrate multiple concepts into a single program. This could be done as part of a lecture assignment or possibly in a lab or recitation context.

Outside of the classroom, when you work on a personally relevant project, you find yourself investing more effort because you find more meaning in the end results and the successes and failures along the way. Development, maintenance, and extension of the project's code is usually more engaging and rewarding than prescribed projects in a classroom setting.

Here are some ideas for various interests as examples of projects you could work on independently:

1. **Sports - Season Stats Dashboard.** Read a small CSV of your favorite team's game results, loop through the rows to compute win/loss record, average points per game, and longest winning streak.  (Optional) Use `matplotlib` to plot points-per-game over time.
2. **Music - Auto-Playlist Generator.** Ask the user for mood + desired length, then pick random tracks from a text file of songs until the total runtime matches.  Reinforces accumulators, while-loops, and string parsing.
3. **Music - Vinyl Collection Analyzer**. Load a CSV of albums, iterate to find the oldest press, most-represented artist, and average track count.  Stretch goal: plot decade distribution with `matplotlib`.
4. **Gaming - Pygame Click-the-Target (Whack-a-Mole).** Use `Pygame` to spawn a circle at random spots every second; the player gains a point by clicking it before it disappears.  Teaches event loops, collision detection, and basic scoring--plus plenty of room for power-ups or difficulty scaling.
5. **Visual Arts - Turtle Mondrian Maker.** Use `Turtle` or `Pygame` to draw random rectangles in primary colors until the canvas fills, mimicking Piet Mondrian's style. Loops, random numbers, and simple functions keep the project around  150 lines of code.
6. **Astronomy - Phases of the Moon Simulator.**  Print or use `Turtle` or `Pygame` to draw the current moon phase based on day-of-cycle input, then advance the phase each keypress. Great for modular arithmetic and simple animation.
7. **Cinema - Movie Night Randomizer.** Load a CSV of films with genre and runtime; ask for desired genre + maximum length and pick a random match. Shows filtering lists of dictionaries and using `random.choice`.
8. **Literature - Choose-Your-Own Adventure Story.** Create a text-based interactive fiction game where users make choices that shape the story.

Below is a sample prompt you could use to have AI help you generate some personal project ideas. You would replace `<STUDENT_INTEREST>` with a field or discipline you're interested in. Once you have a suggested project, if you're not sure where to start you could use techniques suggested in the [assignment guidance](#assigns) section found earlier in this document to get some guidance about where to start.

> You are a CS1 project mentor. Generate **3 personal-project ideas** that satisfy these constraints:
> 
>* Concepts allowed: variables, conditionals, loops, lists, functions, files.
> * Scope: ≤ 250 lines, doable in ~10 hours.
> * Domain personalization: <STUDENT_INTEREST>.
> * For each idea include:
>   1. One-sentence hook
>   2. Brief, more in-depth summary
>   2. 3 incremental milestones
>   3. 2-3 edge-case test suggestions
>   4. Success criteria (what output proves it works)
>   5. One reflection question
> 
> Avoid external libraries unless already mentioned or absolutely necessary, web scraping, or file formats beyond plain text or CSV.

Here are some considerations that you could use to modify the prompt:

Consideration |Why it matters (CS-pedagogy + logistics)|How to include it in the prompt
:--|:--|:--
Concept alignment|Keep ideas inside the zone of proximal development, only use constructs covered so far (e.g., variables → loops → lists → functions).|"Suggest projects that rely only on loops, conditionals, and lists, no classes or external libraries."
Scope & feasibility|A project that balloons to 1,000 LOC defeats motivation. A typical CS1 [timebox](https://en.wikipedia.org/wiki/Timeboxing) for larger projects is about 10-15 hours total.|"Each idea should be completable in < 300 lines or ≤ two weekends of work."
Incremental milestones|Breaking a project into checkpoints improves completion rates and supports formative feedback.|"For each project, list 3-4 numbered milestones students can test after finishing each."
Student agency / interests| [Project-based learning](https://en.wikipedia.org/wiki/Project-based_learning) studies find relevance boosts engagement and retention | "Tailor project themes to a specific student interest."
Authentic data / context|Real-world flavor (CSV files, text logs, simple APIs) cultivates transfer of learning |"Prefer ideas that read small real datasets (e.g., a `weather.csv` file)."
Assessment hooks|Clear success criteria help you know when you're done.|"Include at least two observable outcomes (expected console output or sample tests)."
Testing & debugging practice|Embedding test-driven habits early reinforces debugging.|"For each idea, suggest 3 edge cases I should test."
Accessibility & tooling|Assume only Python standard library and a basic IDE (Thonny).|"No third-party packages; run in vanilla Python 3.11 on Windows/Mac/Linux."
Creativity vs. prescription|Balance open-ended exploration with enough structure that novices aren't paralyzed.|"Offer one guided, one semi-guided, and one open-ended prompt for variety."
Reflection component|Metacognition cements learning.|"End each project brief with one reflection question, e.g., 'What was your biggest debugging hurdle?'"

---
